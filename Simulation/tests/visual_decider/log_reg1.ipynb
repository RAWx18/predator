{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device \n",
    "# Get the GPU device name if available.\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU(s) available: {}'.format(torch.cuda.device_count()))\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 41\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/Dataset_unsupervised.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get to Know the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Camera').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['Camera'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud of text\n",
    "\n",
    "# Get stopwords\n",
    "# Define nltk stopwords in english\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "# Get a string of all the texts available\n",
    "data_text = \",\".join(txt.lower() for txt in df.Command)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_font_size=50, \n",
    "                      max_words=100, \n",
    "                      stopwords=stop_words,\n",
    "                      scale=5,\n",
    "                      background_color=\"white\").generate(data_text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most repeated words in all Commands',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nocam = df[df['Camera']==0]\n",
    "data_text_nocam = \",\".join(txt.lower() for txt in df_nocam.Command)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud_nocam = WordCloud(max_font_size=50, \n",
    "                      max_words=100, \n",
    "                      stopwords=stop_words,\n",
    "                      scale=5,\n",
    "                      background_color=\"white\").generate(data_text_nocam)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud_nocam, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most repeated words in Commands not requiring a camera',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cam = df[df['Camera']==1]\n",
    "data_text_cam = \",\".join(txt.lower() for txt in df_cam.Command)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud_cam = WordCloud(max_font_size=50, \n",
    "                      max_words=100, \n",
    "                      stopwords=stop_words,\n",
    "                      scale=5,\n",
    "                      background_color=\"white\").generate(data_text_cam)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud_cam, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Most repeated words in Commands requiring a camera',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsampling data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cam_downsampled = df_cam.sample(df_nocam.shape[0])\n",
    "df = pd.concat([df_cam_downsampled, df_nocam])\n",
    "df.groupby('Camera').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test and train data using 25% of the dataset for validation purposes\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Command'], df['Camera'], test_size=0.25, shuffle=True, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline with the TfidfVectorizer and LogisticRegression model\n",
    "LR_pipeline = Pipeline(steps = [('tf', TfidfVectorizer()), \n",
    "                                ('lgrg', LogisticRegression())]) # initialize TfidfVectorizer and LogisticRegression\n",
    "\n",
    "\n",
    "# Create Parameter Grid\n",
    "pgrid_lgrg = {\n",
    " 'tf__max_features' : [1000, 2000, 3000],\n",
    " 'tf__ngram_range' : [(1,1),(1,2)],\n",
    " 'tf__use_idf' : [True, False],\n",
    " 'lgrg__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    " 'lgrg__class_weight' : ['balanced', None]\n",
    "}\n",
    "\n",
    "# Apply GridSearch to Pipeline to find the best parameters\n",
    "gs_lgrg = GridSearchCV(LR_pipeline, pgrid_lgrg, cv=2, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lgrg.fit(x_train, y_train) # Train LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lgrg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score of train set', gs_lgrg.score(x_train, y_train))\n",
    "print('Score of test set',gs_lgrg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pred = gs_lgrg.predict(x_test) # Predict on validation data\n",
    "\n",
    "data = {'true_y': y_test,\n",
    "       'predicted_y': LR_pred}\n",
    "df_pred = pd.DataFrame(data, columns=['true_y','predicted_y'])\n",
    "confusion_matrix = pd.crosstab(df_pred['true_y'], df_pred['predicted_y'], rownames=['True'], colnames=['Predicted'])\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of LR model', accuracy_score(y_test, LR_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['true_y', 'predicted_y']\n",
    "print(classification_report(y_test, LR_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(round(gs_lgrg.predict([\"Follow the drone please\"])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the fitted GridSearchCV model\n",
    "joblib.dump(gs_lgrg, 'log_reg_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('log_reg_model.pkl')\n",
    "\n",
    "# You can now use the loaded model to make predictions\n",
    "predictions = loaded_model.predict([\"fly with my friend wearing the red shirt and black pant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lgrg.predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
